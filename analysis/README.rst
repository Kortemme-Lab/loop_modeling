Loop Modeling Metrics
=====================
The benchmark report highlights four metrics which can be used to compare 
different protocols.  These are:

- Percentage of sub-angstrom predictions
- The best RMSD among the lowest 5 scoring predictions
- The best RMSD of the lowest scoring predictions
- Runtime

The percent sub-angstrom metric reflects how often a protocol predicts a 
near-native loop conformation.  Here near-native is defined as being closer 
than 1Å backbone atom RMSD.  This metric is calculated individually for each 
structure in the benchmark based on the 500 (by default) predictions made for 
each loop.  Those individual percentages are then aggregated into a single 
distribution.  We are usually most interested in the median and the third 
quartile of this distribution.  For every method tested so far, the first 
quartile has remained very close to zero (meaning that very few near-native 
predictions are being made).  The big problem with this metric is that it can 
penalize protocols that do a better job of sampling broadly, and which 
consequently spend less time in any particular minima.

The best RMSD metrics are very similar to each other.  The first is the lowest 
RMSD among the 5 lowest scoring predictions, while the second is simply the 
RMSD of the single lowest scoring prediction.  The second metric is probably 
more realistic (because when you're doing loop modeling for real all you have 
is a score) but the first metric is more useful for benchmarking because it is 
considerably less noisy.  These metrics are calculated independently for each 
structure in the benchmark, then aggregated into a single distribution.  We are 
usually interested in the median of this distribution, although we are also 
sometimes interested in the high outliers.  The big problem with these metrics 
are that they can be pretty easily squashed against zero, limiting their 
ability to distinguish between the best loop modeling protocols.

The runtime metric is probably the most intuitive and the least interesting.  
It is simply the average runtime required to make each prediction.  For all the 
rosetta protocols this tends to be very similar, because these protocols spend 
much more time running gradient minimization and side chain optimization than 
than they spend sampling the loop backbone.

Analyzing a Benchmark Run
=========================
The primary script for analyzing benchmark results is ``make_report.py``.  This 
script generates a PDF report on one or more benchmarks.  The command to 
generate a report is::

    ./make_report.py <name of benchmark>...


One or more benchmarks may be specified.  The benchmark name may either be a 
database identifier or the path to a flat file (see the section on input 
formats below).  You can verify that this script is working by creating a 
report based on the sample input files found in ``output/sample``::

    ./make_report.py ../output/sample/*.results

If two or more benchmarks are being considered, the report will begin with a 
section with bow-and-whisker plots comparing the 4 metrics discussed above for 
each benchmark.  If only one benchmark is being considered, this section will 
be omitted.  The report will also contain one section devoted to each 
benchmark.  Each of these sections will contain box-and-whisker plots for the 
"percent sub-angstrom" and "best RMSD lowest 5" metrics, a table summarizing 
the rest of the metrics, a break down of all the metrics for each structure in 
the benchmark, and score vs RMSD plots for each prediction made.  In other 
words, every piece of data generated by the benchmark is included in this 
report.  Each benchmark is also color coded to make it easier to stay oriented.

The other scripts in this directory may be peripherally useful:

show_benchmarks.py
  List the names of all the benchmarks found in the specified database.  These 
  names can be passed to ``make_report.py`` to make reports.

show_log.py
  Show the log files for particular jobs.  This is really only useful if you're 
  trying to debug a protocol.

Dependencies
-------------
The ``make_report.py`` script requires:

- pdflatex with booktabs, graphicx, fullpage, hyperref
- gnuplot
- `klab <https://github.com/Kortemme-Lab/klab/>`_

Input Formats
-------------
The analysis script can accept data either from a relational database or from a 
flat file:

Database schema
...............
The database schema is defined in ``libraries/database.py``.  Note that a large 
fraction of the schema is part of the rosetta "features reporter" framework, 
and as such may be rather difficult for other protocols to mimic.

Flat file format
................
The flat file should be a tab-separated file. The file must have the extension
".results". The first line in the file should be a header line::

  #PDB	Model	Loop_rmsd	Total_energy	Runtime

where the loop RMSD is measured in Ångströms, the energy is measured in the 
units of the loop modeling protocol, and the runtime is measured in seconds.  
Subsequent lines in the file should be data following this format *e.g.*::

  1a8d	1	6.4012	-496.689	2310
  1a8d	2	5.82274	-505.773	3444
  1a8d	3	5.01459	-504.018	4635
  ...
  1a8d	500	5.58601	-501.161	5071
  1arb	1	2.51994	-289.462	4684
  ...

